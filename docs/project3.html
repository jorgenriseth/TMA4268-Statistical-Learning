<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Håvard Bjørkøy, Oliver Byhring and Jørgen Riseth" />


<title>Compulsory Exercise 3</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">TMA4268 - Statistical Learning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="project1.html">Exercise 1</a>
</li>
<li>
  <a href="project2.html">Exercise 2</a>
</li>
<li>
  <a href="project3.html">Exercise 3</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Compulsory Exercise 3</h1>
<h3 class="subtitle">TMA4268 Statistical Learning V2018</h3>
<h4 class="author">Håvard Bjørkøy, Oliver Byhring and Jørgen Riseth</h4>

</div>


<div id="task-1---classification-with-trees" class="section level1">
<h1>Task 1 - Classification with trees</h1>
<div id="a-full-classification-tree" class="section level2">
<h2>a) Full Classification Tree</h2>
<p><code>fulltree</code> is constucted by the <em>recursive binary splitting</em> approach. Start with dividing the predictor space in two half-regions. This is done by testing for all predictors <span class="math inline">\(i\)</span> and cut-off values <span class="math inline">\(s\)</span>, to find the two half planes <span class="math inline">\(R_1(i, s) = \{X|X_i&lt;s\}\)</span> and <span class="math inline">\(R_2(i, s) = \{X|X_i\geq s\}\)</span> which gives the greates reduction in the <em>deviance</em>: <span class="math display">\[
-2 \sum_{k=1}^K n_{jk} \log \hat{p}_{jk}
\]</span> where <span class="math inline">\(\hat{p}_{jk}=n_{jk}/N_j\)</span> is the proportion of training observations of class <span class="math inline">\(k\)</span> in region <span class="math inline">\(j\)</span>. Then perform similar splits recursively for each of the defined regions, until the regions are sufficiently small.</p>
<p>This approach constructs a binary tree, in which the <em>root</em> corresponds to the whole predictor space. The internal nodes corresponds to the distinct splits of the predictor space, and the <em>terminal nodes</em> or <em>leaves</em> corresponds to the final regions in which we’ve separated the predictor space. It’s a <em>greedy</em> algortihm in which the regions are split accoding to the optimal choice at the “current” state, instead of considering if another split would have resulted in a better choice in the future.</p>
</div>
<div id="b-pruned-classification-trees" class="section level2">
<h2>b) Pruned Classification Trees</h2>
<p>While the approach in <strong>a)</strong> works well for the training data, the complexity of the tree may cause overfitting, and reduce the interpretability of the data. In such situations, a smaller tree containing fewer splits might both increase the understandability of the model, and lower the test error. Given a full tree built by recursive binary splitting, select a subtree which should minimize the test error rate. This is called <em>prunning</em> of the tree.</p>
<p>The amount of pruning given in the code is decided by <em>cost complexity pruning</em>. The function <code>prune.misclass</code> finds a sequence of subtrees it minimizes the cost-complexity function: <span class="math display">\[
C_\alpha(T) = \frac{1}{n}\sum_i I(y_i \neq \hat{y}_k) + \alpha |T|
\]</span> for a given non-negative parameter <span class="math inline">\(\alpha\)</span>. Then <code>cv.tree</code> estimates the test error on all of these trees using 5-fold cross validation. The size of the pruned tree is chosen as the one with the lowest CV-error rate, which is 4 as apparent from the given figures.</p>
<p>The full tree has a misclassification rate of 0.244, and an AUC of 0.7446, whilst for the the pruned tree the AUC is 0.7171, and the misclassification rate is 0.26. In other wods, the full tree scores better than the pruned one, but what is lost in performance, is gained in interpetability: While the full tree with a a total of 32 nodes, 15 of which are leaf nodes, is almost unreadable, the pruned tree with it’s three splits and four terminal nodes is very easy to follow.</p>
</div>
<div id="c-bagged-trees" class="section level2">
<h2>c) Bagged Trees</h2>
<p>Simple decision trees have a downside: They are non-robust with high variance, and a small change in data may cause a large change in the final tree. Thus the the result may be quite different for different splits between test and training data. <em>Boostrap aggregation</em> or <em>bagging</em> is a procedure to reduce variance in a statistical learning method, and is especially useful for decision trees.</p>
<p>Since bagged classification trees are based on a set of <span class="math inline">\(B\)</span> different trees, it lacks interpratibility and simple visualization of the simple decision tree. The <em>variable importance plots</em> displays the importance of splits based on the different variables. The left plot computes variable importance based on OOB error, an estimate of the test misclassification rate in which the <span class="math inline">\(i\)</span>’th observation is classified by the majority vote from the trees in which the observation was <em>out-of-bag</em>, i.e. not among the observations used to fit the tree. In the right plot the variable importance is computed by the mean decrease in the Gini index, or the increase in node purity for the variable splits.</p>
<p>As stated earlier, bagged trees lose some of the interpretability of the simple decision trees. While the variable importance plots recovers some of the interpratibility, it’s still got a long way to the intuitive display of both the full and pruned decision tree. What the bagged trees lack in interpretability, it makes up for in performance. With a higher accuracy and an AUC of 0.8158 it surpasses both the previous trees with a signficant amount.</p>
</div>
<div id="d-random-forest" class="section level2">
<h2>d) Random Forest</h2>
<pre class="r"><code>plot(testbagroc, colorize = T)
plot(testrfroc, add = T, col = 2)</code></pre>
<p><img src="project3_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>While decision trees are non-robust, and you might get quite different trees while building bagged trees, most of them will use a strong predictor in the to split. As these trees will be higly correlated, we won’t get the desired variance redution. Random forests try to fix this problem by only considering a randomly chosen subset of the overall predictors at each split. The <code>mtry=4</code> parameter passed to the <code>randomForest</code> function is the number of parameters considered, and for classification trees this is usually chosen to be <span class="math inline">\(m \approx \sqrt{p}\)</span>, where <span class="math inline">\(p\)</span> is the number of predictors. For the credit dataset, we round down from <span class="math inline">\(\sqrt{20} \approx 4.47\)</span>. As stated above, this approach decorrrelates the individual trees built from a bagging approach, and should therefore lead to a further reduction in variance. While the prediction accuracy on the test data is similar for bagging and random forest, the random forest approach has an AUC of 0.8358 somewhat higher than for the bagged trees.</p>
<p>In the case of the credit risk data, wrongfully classifying a bad credit risk as good is worse than the opposite. Therefore it could be smart to set the class-boundary such that we get a high specificity. In the figure above the ROC-curve for bagged trees in black is plotted against the ROC for random forest. For most values they are almost the same, but for an interval around a specificity of 0.75, the random forest approach is higher, and thus random forests is our preferred approach for this dataset.</p>
</div>
</div>
<div id="task-2---nonlinear-class-boundaries-and-support-vector-machines" class="section level1">
<h1>Task 2 - Nonlinear class boundaries and support vector machines</h1>
<div id="a" class="section level2">
<h2>a)</h2>
<p>The Bayes classifier is a simple rule of how classify a certain test observation with predictor vector <span class="math inline">\(x_0\)</span>. Given an observation, the rule is to assign the observation to the class of largest probability, i.e. selecting the class <span class="math inline">\(j\)</span> for which <span class="math inline">\(Pr(Y = j | X = x_0)\)</span> is largest. Having made some rule from training data, we use this information to compute the conditional probability. This results in that the Bayes classifier is the method that has least probability of misclassifying. It does not matter how many classes we introduce - the largest one always win.</p>
<p>The Bayesian decision boundary arises where probabilities of observing two classes are equal and largest, so it separates regions where we would classify differently. If <span class="math inline">\(Pr(Y = i | X=x_0) = Pr(Y = j|X = x_0)\)</span> are the largest value of all <span class="math inline">\(i,j, i \neq j\)</span>, we have a boundary at <span class="math inline">\(x_0\)</span>. Two equal probabilities that are not the largest will not define this boundary. Having two variables this line will always be 50 % probability of observing either class. The Bayes decision boundary draws a hypersurface/multidimensional map for which classifications the classifier make.</p>
<p>Since the Bayes classifier has the lowest probability of misclassifying, the Bayes error rate (BER) is the lowest test error rate there is. We can use the expected prediction values of our whole set to find this test error rate. In a point <span class="math inline">\(x_0\)</span> we have a probability <span class="math inline">\(p = max_j \{Pr(Y = j | X = x_0)\}\)</span> of classifying correctly, i.e. the misclassification probability is <span class="math inline">\(1-p\)</span>, so for our entire set <span class="math inline">\(X\)</span>, <span class="math inline">\(BER = 1 - E(max_j(Y = j|X))\)</span>.</p>
<p>A good reason to have a test set would be to measure the error rate for the population we’re classifying. Even though we know that the BER is the lowest error rate, we do not know what it is, i.e. how presice we are classifying. If we do not care for the rate, and we by chance know the Bayes boundary, there is no reason to find another boundary - it won’t be better on the test set.</p>
</div>
<div id="b" class="section level2">
<h2>b)</h2>
<p>The difference between a support vector machine (SVM) and a support vector classifier (SVC) is that the last one is a generalization of the first. SVC is a SVM that only allows linear decision boundaries, making it a hyperplane, and is also called a soft margin classifier. SVM can take on non-linear forms.</p>
<p>SVM and the SVC creates a classification boundary, both using a soft-margin technique, where observations are separated by a surface trying to maximize its margin to nearby observations. The soft part comes from introducing some slack for misclassification, allowing the boarder to misclassify according to the slack, which is a tuning parameter. In the SVM function in R the parameter is called <span class="math inline">\(cost\)</span>, which is tuneable using <code>tune</code> (grid search). Other parameters consist of <code>kernel</code>, which specifies the shape of the SVM - linear or not. There are many kernels to choose from, here they use the common radial basis functions kernel. For non-linear boundaries we would want to specify the curvature, or rather how far a single training example can affect our boundary. This is controlled by the tuning parameter <code>gamma</code>, where small <code>gamma</code> would result in a flatter shape, and large values might overfit the surface to high complexity. The relation is <span class="math inline">\(\gamma = \frac{1}{2\sigma^2}\)</span>, the inverse of the variance, which gives an explanation to the low variance when overfitting. This is also tuneable by grid search. The grid search in <code>tune</code> is done by 10-fold cross-validation for all combinations of tuning parameters on constants splits in the data set. The best combination of lowest error rate is chosen. In the exercise we see that the optimal values for these tuning parameters are <span class="math inline">\(cost = 1, gamma = 5\)</span>.</p>
<p>The evaluation of the SVM decision boundary should be compared using the error rate compared to the BER. We want our error rate close to the BER, but if it is lower, we might expect some overfitting, since the BER should be the lowest test error rate. The lines appear to be very similar, but maybe this curve is too wiggly for the SVM, and we might want to try other values of <code>gamma</code> and <code>cost</code> nearby the “optimal” ones, since the SVM boundary seems to perform slightly better here. In addition, there is a small elliptic boundary in the upper left, which is strange.</p>
</div>
</div>
<div id="task-3---unsupervised-methods" class="section level1">
<h1>Task 3 - Unsupervised methods</h1>
<div id="a-1" class="section level2">
<h2>a)</h2>
<p>The biplot is a way of combining info about all observations values, and visualizing them in two dimensions. As we see from the vectors in the biplot is that the first principal component mostly consists of tea, wine and beer, where wine has negative effect, and the other two positive. This is visible from reading off their values on the first upper axis, corresponding to their weight in PC1. The principal components are constructed to point in the most varying direction, i.e. explain the most varying combination of all variables. The second principal component is mostly made up from liquer (negative), coffee and cocoa. The last two weigh positive. Now knowing what the two axis explain we can start interprating the placement of each countries by reading off their values of the two components on the opposing axis’.</p>
<p>From inspecting the summary, in particular the vector components of PC1 and PC2, we see that the observations obviously coincide with the observations from the biplot. The two vectors consist of a weighted combination of each variable, but some of them has greater impact on the value of the principal component.</p>
<p>This analysis does really tell us something about drinking habits for certain countries and the similarities. For instance, Poland, Soviet Union and Hungary all have negative values of PC2, but seem quite neutral in PC1 - wine, tea and beer. The most significant negative factor is liquer, implying that these three countries have a similarity in high liquer consumption. PC1 being neutral could both mean that the consumption is high, but balanced, or a general low consumption. Other countries with negative PC2 values are also most probable placed there because of large liquer consumption. The components of PC1 have a more similar weight, forcing us to generalize more, but there are definately similarities. The negative values of Italy and Spain is probably because of large wine consumption, and Great Britain and Ireland most likely are similar in their consumption of tea and beer.</p>
</div>
</div>
<div id="b-1" class="section level1">
<h1>b)</h1>
<p>The distance between clusters for a single linkage is the smallest distance between two clusters, where the cluster can be either one or several points. Here, distance is not defined because there are several methods to measure it, for instance the intuitive euclidean distance, or correlation-based distance. For complete linkage the distance between two clusters is the distance between two elements in different clusters that are the furthest away from each other. We say it uses maximal intercluster dissimilarity. The average linkage makes use of the centroid of each cluster, and the distance between clusters is the distance between their centroids.</p>
<p>We could not decide which figure corresponds to which measure of distance by the lowest level, where all measures give the same lowest distance, though comparing the first cluster (Human and chimpanzee) with their link to gorilla, we could tell them apart. From the table we see that the distance between chimpanzee and human is 1, and their centroid is <span class="math inline">\(\frac{0+1}{2} = 0.5\)</span>. The new lowest average distance would be the one from the centroid to the gorilla, which is obviously <span class="math inline">\(2.5\)</span>, hence the figure B is the average linkage, with gorilla on level <span class="math inline">\(2.5\)</span>. The maximal intercluster dissimilarity is from the lowest cluster to gorilla, i.e. <span class="math inline">\(max{|0-3|, |1-3|}=3\)</span>, while the minimal intercluster dissimilarity is 2 - distance between chimpanzee and gorilla. Figure A has placed gorilla on height 3, which indicates that this is the complete linkage tree, while figure C has gorilla on height 2, which corresponds to the single linkage measure. For a large tree, the single linkage might produce very unbalanced trees compared to the other two methods.</p>
</div>
<div id="task-3---unsupervised-methods-1" class="section level1">
<h1>Task 3 - Unsupervised methods</h1>
<div id="a-2" class="section level2">
<h2>a)</h2>
<p>The biplot is a way of combining info about all observations values, and visualizing them in two dimensions. As we see from the vectors in the biplot is that the first principal component mostly consists of tea, wine and beer, where wine has negative effect, and the other two positive. This is visible from reading off their values on the first upper axis, corresponding to their weight in PC1. The principal components are constructed to point in the most varying direction, i.e. explain the most varying combination of all variables. The second principal component is mostly made up from liquer (negative), coffee and cocoa. The last two weigh positive. Now knowing what the two axis explain we can start interprating the placement of each countries by reading off their values of the two components on the opposing axis’.</p>
<p>From inspecting the summary, in particular the vector components of PC1 and PC2, we see that the observations obviously coincide with the observations from the biplot. The two vectors consist of a weighted combination of each variable, but some of them has greater impact on the value of the principal component.</p>
<p>This analysis does really tell us something about drinking habits for certain countries and the similarities. For instance, Poland, Soviet Union and Hungary all have negative values of PC2, but seem quite neutral in PC1 - wine, tea and beer. The most significant negative factor is liquer, implying that these three countries have a similarity in high liquer consumption. PC1 being neutral could both mean that the consumption is high, but balanced, or a general low consumption. Other countries with negative PC2 values are also most probable placed there because of large liquer consumption. The components of PC1 have a more similar weight, forcing us to generalize more, but there are definately similarities. The negative values of Italy and Spain is probably because of large wine consumption, and Great Britain and Ireland most likely are similar in their consumption of tea and beer.</p>
<p>#b) The distance between clusters for a single linkage is the smallest distance between two clusters, where the cluster can be either one or several points. Here, distance is not defined because there are several methods to measure it, for instance the intuitive euclidean distance, or correlation-based distance. For complete linkage the distance between two clusters is the distance between two elements in different clusters that are the furthest away from each other. We say it uses maximal intercluster dissimilarity. The average linkage makes use of the centroid of each cluster, and the distance between clusters is the distance between their centroids.</p>
<p>We could not decide which figure corresponds to which measure of distance by the lowest level, where all measures give the same lowest distance, though comparing the first cluster (Human and chimpanzee) with their link to gorilla, we could tell them apart. From the table we see that the distance between chimpanzee and human is 1, and their centroid is <span class="math inline">\(\frac{0+1}{2} = 0.5\)</span>. The new lowest average distance would be the one from the centroid to the gorilla, which is obviously <span class="math inline">\(2.5\)</span>, hence the figure B is the average linkage, with gorilla on level <span class="math inline">\(2.5\)</span>. The maximal intercluster dissimilarity is from the lowest cluster to gorilla, i.e. <span class="math inline">\(max{|0-3|, |1-3|}=3\)</span>, while the minimal intercluster dissimilarity is 2 - distance between chimpanzee and gorilla. Figure A has placed gorilla on height 3, which indicates that this is the complete linkage tree, while figure C has gorilla on height 2, which corresponds to the single linkage measure. For a large tree, the single linkage might produce very unbalanced trees compared to the other two methods.</p>
</div>
</div>
<div id="task-4---neural-networks" class="section level1">
<h1>Task 4 - Neural networks</h1>
<p>A non-linear activation function such as relu is used to capture more complex patterns in the data. Without them each layer would only be able to learn linear transformations of the input data and a deep stack of linear layers would still implement a linear operation. The activation function relu add non-linearity to the model.</p>
<p>Classifiying a review as good or bad is a binary classification. Therefore we need to use binary_crossentropy as loss function. This function is activated when we use the sigmoid last-layer activation function. We cannot use the relu function in the output layer since we do not know its max value and can therefore not choose a classification boundry.</p>
<pre class="r"><code>rm(list=ls())
library(keras)
library(magrittr)

# Data preparation
imdb &lt;- dataset_imdb(num_words = 10000)

train_data &lt;- imdb$train$x
train_labels &lt;- imdb$train$y
test_data &lt;- imdb$test$x
test_labels &lt;- imdb$test$y

word_index &lt;- dataset_imdb_word_index()  
reverse_word_index &lt;- names(word_index)                                    
names(reverse_word_index) &lt;- word_index
decoded_review &lt;- sapply(train_data[[1]], function(index) {                
  word &lt;- if (index &gt;= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word else &quot;?&quot;
})


# Turn data to tensor format
vectorize_sequences &lt;- function(sequences, dimension = 10000) {
  results &lt;- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] &lt;- 1                                     
  results
}

x_train &lt;- vectorize_sequences(train_data)
x_test &lt;- vectorize_sequences(test_data)

y_train &lt;- as.numeric(train_labels)
y_test &lt;- as.numeric(test_labels)


# Define and compile models

model.simple &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 4, activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;%
  layer_dense(units = 4, activation = &quot;relu&quot;) %&gt;%
  layer_dense(units = 1, activation = &quot;sigmoid&quot;)

model.complex &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 32, activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;%
  layer_dense(units = 32, activation = &quot;relu&quot;) %&gt;%
  layer_dense(units = 1, activation = &quot;sigmoid&quot;)

model.simple %&gt;% compile(
  optimizer = &quot;rmsprop&quot;,
  loss = &quot;binary_crossentropy&quot;,
  metrics = c(&quot;accuracy&quot;)
)

model.complex %&gt;% compile(
  optimizer = &quot;rmsprop&quot;,
  loss = &quot;binary_crossentropy&quot;,
  metrics = c(&quot;accuracy&quot;)
)


# Approach validation
val_indices &lt;- 1:10000

fit.simple &lt;- model.simple %&gt;% fit(
  x_train[-val_indices,], 
  y_train[-val_indices],
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_train[val_indices,], y_train[val_indices])
)

fit.complex &lt;- model.complex %&gt;% fit(
  x_train[-val_indices,], 
  y_train[-val_indices],
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_train[val_indices,], y_train[val_indices])
)


plot(fit.simple)</code></pre>
<p><img src="project3_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>plot(fit.complex)</code></pre>
<p><img src="project3_files/figure-html/unnamed-chunk-5-2.png" width="672" /> For the 16-layer model the validation accuracy reaches its maximum after 5 epochs, where the accuracy attains the value 0.889. The simpler model using 4 units in the hidden layers, attains its maximum validation accuracy of 0.8766 after 9 epochs, but with a significantly higher loss value. The complex model using 32 units attains its maximum validation accuracy of 0.8875 after 4 epochs. Still the 6 first epochs all seems to be around the same value, so the loss functions start to increase from the first epoch, which indicates that the model is too complex.</p>
<p>There are four common ways to prevent overfitting in neural networks besides reducing network complexity:</p>
<ol style="list-style-type: decimal">
<li><strong>More data</strong>: If it’s possible to get more data, making sure there are more samples than the number of parameters in the model is a way to fight overtfitting.</li>
<li><strong>Weight regularization</strong>: Weight regularization is similar to the different <em>information criterions</em> presented in chapter 6 to choose between linear models. The idea is to penalize increased network complexity during training, by adding the weight values multiplied by a regularization coefficient to the total loss of the network.</li>
<li><strong>Dropout</strong>: By randomly dropping out i.e. setting output features to zero while training the network, random patterns in the data may be broken up by the introduced noise of the droupout. When testing the model on test data, the output values of the layers are scaled down by the <em>dropout factor</em>, the the fraction of features dropped.</li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
